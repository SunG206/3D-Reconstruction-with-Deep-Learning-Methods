{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SunG206/3D-Reconstruction-with-Deep-Learning-Methods/blob/master/GPT_NeoX_20B_on_Flax_(xmap).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸš¨ Caution: Don't use this notebook for research. ðŸš¨\n",
        "\n",
        "<p>The Flax implementation on TPUs currently has a slight performance regression relative to the PyTorch implementations. The comparison can be seen <a href=\"https://docs.google.com/spreadsheets/d/16dhItf9iWMQsYqhcH6wUwZyi-Bf71Nr0TNTVvwAXGRo/edit?usp=sharing\">here</a>.</p>\n",
        "\n",
        "<p>If you want to evaluate GPT-NeoX-20B for research purposes, please use the original <a href=\"\">GPT-Neox</a>, <a href=\"https://github.com/zphang/minimal-gpt-neox-20b\">Minimal PyTorch<a/> or <a href=\"https://huggingface.co/EleutherAI/gpt-neox-20b\">Hugging Face</a> implementations.</p>\n",
        "\n",
        "<p>This TPU implementation of GPT-NeoX-20B is also still a prototype with some hacks, so if you see any room for improvement, please drop by <a href=\"https://github.com/zphang/minimal-gpt-neox-20b\">the repo</a>!</p>\n",
        "\n",
        "(For instance, I'm resorting to fp32 for some operations to avoid NaNs, which leads to greater memory usage than is necessary.)"
      ],
      "metadata": {
        "id": "miaD6_jlORRK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf minimal-gpt-neox-20b\n",
        "!git clone https://github.com/zphang/minimal-gpt-neox-20b.git"
      ],
      "metadata": {
        "id": "aELVip9uJYXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r minimal-gpt-neox-20b/requirements_flax.txt"
      ],
      "metadata": {
        "id": "IcAeIg0IKrMX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download tokenizer\n",
        "!wget https://mystic.the-eye.eu/public/AI/models/GPT-NeoX-20B/slim_weights/20B_tokenizer.json -P 20B_checkpoint\n",
        "# Download weights (takes a couple minutes, usually ~5 min)\n",
        "!cat ./minimal-gpt-neox-20b/assets/weights_urls.txt | xargs -n 1 -P 8 wget -P 20B_checkpoint -q"
      ],
      "metadata": {
        "id": "tVtfK5q9obc4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import jax.tools.colab_tpu\n",
        "jax.tools.colab_tpu.setup_tpu()"
      ],
      "metadata": {
        "id": "hAwLCHOOubNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path += [\"./minimal-gpt-neox-20b\"]\n",
        "from flax.core import frozen_dict\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "import tokenizers\n",
        "from minimal20b_flax import create, model_xmap, generate\n",
        "\n",
        "import IPython\n",
        "\n",
        "def show(input_string, output_string):\n",
        "  display(IPython.display.HTML(\"<div style='width:800px; font-family: monospace;'>{}<b style='color:orange;'>{}</b></div>\".format(input_string, output_string)))\n",
        "\n",
        "model_xmap.CACHED_FUNCS = {}"
      ],
      "metadata": {
        "id": "lLe4aGOcKrQB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading weights (~10-15 min)\n",
        "params = create.colab_load_model_weights_for_xmap(\"./20B_checkpoint/\",)"
      ],
      "metadata": {
        "id": "EAOakHzkKzpL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = tokenizers.Tokenizer.from_file(\"./20B_checkpoint/20B_tokenizer.json\")\n",
        "neox_model = model_xmap.GPTNeoX20BModel(\n",
        "    generate_length=100,  # How many tokens to generate\n",
        "    sampler_args=frozen_dict.freeze({\"temp\": 1.})  # Sampling temperature\n",
        ")"
      ],
      "metadata": {
        "id": "3BfES5OQKzso"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_string = \"\"\"GPT-NeoX-20B is a 20B-parameter autoregressive Transformer model developed by EleutherAI.\"\"\""
      ],
      "metadata": {
        "id": "IKxxerZZZ5gC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# JIT-ing the first inference call takes a while (~3 min), and it will re-JIT\n",
        "# every time the context/output lengths/temperature are modified. Otherwise,\n",
        "# on subsequent calls the model should run much more quickly (~20s maybe?)\n",
        "output = generate.generate(\n",
        "    input_string=input_string,\n",
        "    neox_model=neox_model,\n",
        "    params=params,\n",
        "    tokenizer=tokenizer,\n",
        ")"
      ],
      "metadata": {
        "id": "bsVNp2zCZ5kx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show(input_string, output[\"generated_string\"])"
      ],
      "metadata": {
        "id": "FtHCobktGJNG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}